很好，地基已经扎实：Stage1 分割（实例多通道）+ µScale + 固定划分 + resume/早停 + 主动学习都跑通了。下一步按计划进入 Stage2 轻量 Teacher（ROI 多头分类），给后续一体化 Mask2Morph 提供蒸馏用的软标签，并且让你在小数据阶段先把五头分类效果稳定起来。

本步目标
- 用上一步导出的 ROI（rois_train_epochX/ 下的 rois.json 与裁剪 png）训练轻量 Teacher（ResNet50 编码 + 5 个分类头）。
- 标签映射严格对齐 YAML（cell_org 7/8/9/44→0/1/2/3，旧 10/11→44；其余维��按映射）。
- 小数据友好：加权采样（按 species）、label smoothing、resume/早停、固定划分。
- 导出 Teacher 的 logits（五头）到 .npz 供后续 Mask2Morph 蒸馏（KL）使用。
- 输出混淆矩阵（npy + png）和日志。

你已有的文件可直接复用
- datasets/roi_multitask_dataset.py（如果尚未创建，我下面用到它；你也可复制粘贴我之前给的版本）
- models/multitask_heads.py（ResNet50 编码 + 5头），utils/metrics_cls.py

一、训练脚本 tools/train_stage2_teacher.py（升级版：加权采样/label smoothing/early stop/resume/混淆矩阵可视化）
保存为 tools/train_stage2_teacher.py

```python
# -*- coding: utf-8 -*-
import os, argparse, numpy as np, torch, torch.nn as nn
from torch.utils.data import DataLoader, random_split, WeightedRandomSampler
from ruamel.yaml import YAML

from datasets.roi_multitask_dataset import ROIMultiTaskDataset
from models.multitask_heads import MultiHeadClassifier
from utils.metrics_cls import accuracy, macro_f1, confusion_matrix

# ============== Utils ==============
def set_seed(seed=42):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

class EarlyStopping:
    def __init__(self, patience=20, min_delta=1e-4):
        self.patience=patience; self.min_delta=min_delta
        self.best=None; self.count=0; self.should_stop=False
    def step(self, metric):
        if self.best is None or metric > self.best + self.min_delta:
            self.best = metric; self.count = 0
        else:
            self.count += 1
            if self.count >= self.patience:
                self.should_stop=True

class LabelSmoothingCE(nn.Module):
    def __init__(self, eps=0.1):
        super().__init__(); self.eps=eps; self.logsoftmax=nn.LogSoftmax(dim=1)
    def forward(self, logits, target):
        n = logits.size(1)
        logp = self.logsoftmax(logits)
        with torch.no_grad():
            true_dist = torch.zeros_like(logp)
            true_dist.fill_(self.eps / (n - 1))
            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.eps)
        return torch.mean(torch.sum(-true_dist * logp, dim=1))

def plot_cm_png(cm: np.ndarray, labels: list, out_path: str, normalize=True):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(6,5))
    M = cm.astype(np.float32)
    if normalize:
        row_sum = M.sum(axis=1, keepdims=True) + 1e-9
        M = M / row_sum
    im = plt.imshow(M, cmap='Blues')
    plt.colorbar(im, fraction=0.046, pad=0.04)
    plt.xticks(ticks=np.arange(len(labels)), labels=labels, rotation=45, ha='right')
    plt.yticks(ticks=np.arange(len(labels)), labels=labels)
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()

def compute_species_weights(ds):
    from collections import Counter
    cnt = Counter()
    for i in range(len(ds)):
        _, labels, _ = ds[i]
        cnt[int(labels['species'])] += 1
    total = sum(cnt.values())
    # 逆频率
    class_w = {k: total / (v + 1e-6) for k, v in cnt.items()}
    weights = []
    for i in range(len(ds)):
        _, labels, _ = ds[i]
        weights.append(class_w[int(labels['species'])])
    return torch.DoubleTensor(weights)

# ============== Train/Eval ==============
def train_one_epoch(model, loader, optim, device, criterions, log_every=50):
    model.train()
    ls_meter=[]
    for it, (imgs, labels, meta) in enumerate(loader):
        imgs = imgs.to(device, non_blocking=True)
        sp = labels['species'].to(device)
        co = labels['cell_org'].to(device)
        sh = labels['shape'].to(device)
        fl = labels['flagella'].to(device)
        ch = labels['chloroplast'].to(device)

        outs = model(imgs)
        # 5头 CE（带label smoothing）
        ce = criterions
        loss = (ce(outs['species'], sp) +
                ce(outs['cell_org'], co) +
                ce(outs['shape'], sh) +
                ce(outs['flagella'], fl) +
                ce(outs['chloroplast'], ch))
        optim.zero_grad(set_to_none=True)
        loss.backward()
        optim.step()
        ls_meter.append(loss.item())
        if (it+1) % log_every == 0:
            print(f"  iter {it+1}/{len(loader)} loss={np.mean(ls_meter):.4f}")
    return float(np.mean(ls_meter)) if ls_meter else 0.0

@torch.no_grad()
def evaluate(model, loader, device, out_dir=None, class_names=None):
    model.eval()
    heads = {
        'species': 45, 'cell_org': 4, 'shape': 10, 'flagella': 5, 'chloroplast': 4
    }
    stats = {k: {'acc':0,'f1':0,'cm':np.zeros((C,C),dtype=np.int64)} for k, C in heads.items()}
    total = 0
    for imgs, labels, meta in loader:
        imgs = imgs.to(device)
        sp = labels['species'].to(device)
        co = labels['cell_org'].to(device)
        sh = labels['shape'].to(device)
        fl = labels['flagella'].to(device)
        ch = labels['chloroplast'].to(device)
        outs = model(imgs)
        for k, targ, C in [('species',sp,45),('cell_org',co,4),('shape',sh,10),('flagella',fl,5),('chloroplast',ch,4)]:
            acc = accuracy(outs[k], targ)
            f1  = macro_f1(outs[k], targ, C)
            cm  = confusion_matrix(outs[k], targ, C)
            stats[k]['acc'] += acc * imgs.size(0)
            stats[k]['f1']  += f1  * imgs.size(0)
            stats[k]['cm']  += cm
        total += imgs.size(0)
    for k in stats.keys():
        stats[k]['acc'] /= total
        stats[k]['f1']  /= total

    if out_dir:
        os.makedirs(out_dir, exist_ok=True)
        for k in stats.keys():
            np.save(os.path.join(out_dir, f"cm_{k}.npy"), stats[k]['cm'])
            # 可视化 PNG
            try:
                labs = [str(i) for i in range(stats[k]['cm'].shape[0])]
                plot_cm_png(stats[k]['cm'], labs, os.path.join(out_dir, f"cm_{k}.png"))
            except Exception as e:
                print(f"[warn] plot cm failed for {k}: {e}")
    # 综合指标（用于 early stopping）：五头acc的平均
    mean_acc = np.mean([stats[k]['acc'] for k in stats.keys()])
    return stats, float(mean_acc)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--cfg", required=True, type=str)
    ap.add_argument("--roi_root", required=True, type=str, help="包含 rois.json 或其子目录的根目录")
    ap.add_argument("--epochs", type=int, default=40)
    ap.add_argument("--batch_size", type=int, default=64)
    ap.add_argument("--lr", type=float, default=5e-4)
    ap.add_argument("--weight_decay", type=float, default=1e-4)
    ap.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    ap.add_argument("--roi_size", type=int, default=224)
    ap.add_argument("--out_dir", type=str, default="runs/stage2_teacher")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--weighted_sample", type=int, default=1)
    ap.add_argument("--resume", type=str, default=None)
    ap.add_argument("--patience", type=int, default=15)
    ap.add_argument("--label_smooth", type=float, default=0.1)
    args = ap.parse_args()

    set_seed(args.seed)
    yaml = YAML()
    with open(args.cfg, 'r', encoding='utf-8') as f:
        cfg = yaml.load(f)
    mean = cfg['dataset_stats']['mean']; std = cfg['dataset_stats']['std']

    ds = ROIMultiTaskDataset(roi_root=args.roi_root, cfg_path=args.cfg, roi_size=args.roi_size, mean=mean, std=std)
    # 固定切分（8:2 或根据你 splits/ 也可手动构造 train/val JSON）
    n_total = len(ds); n_val = max(1, int(n_total * 0.2)); n_train = n_total - n_val
    ds_train, ds_val = random_split(ds, [n_train, n_val], generator=torch.Generator().manual_seed(args.seed))

    if args.weighted_sample:
        wts = compute_species_weights(ds_train.dataset if hasattr(ds_train,'dataset') else ds_train)
        sampler = WeightedRandomSampler(weights=wts, num_samples=len(wts), replacement=True)
        dl_train = DataLoader(ds_train, batch_size=args.batch_size, sampler=sampler, num_workers=4, pin_memory=True)
    else:
        dl_train = DataLoader(ds_train, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)
    dl_val   = DataLoader(ds_val,   batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)

    device = torch.device(args.device)
    model = MultiHeadClassifier(pretrained=True).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    ce = LabelSmoothingCE(eps=args.label_smooth).to(device)

    os.makedirs(args.out_dir, exist_ok=True)
    log_path = os.path.join(args.out_dir, "log.csv")
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write("epoch,train_loss,val_mean_acc,val_species_acc,val_cell_org_acc,val_shape_acc,val_flagella_acc,val_chloro_acc\n")

    start_epoch = 1; best_mean_acc = -1
    if args.resume and os.path.exists(args.resume):
        ckpt = torch.load(args.resume, map_location=device)
        model.load_state_dict(ckpt['model']); optimizer.load_state_dict(ckpt['optim'])
        start_epoch = ckpt.get('epoch', 0) + 1
        print(f"[resume] from {args.resume}, next epoch {start_epoch}")

    early = EarlyStopping(patience=args.patience, min_delta=1e-4)

    for epoch in range(start_epoch, args.epochs+1):
        print(f"\nEpoch {epoch}/{args.epochs}")
        tr_loss = train_one_epoch(model, dl_train, optimizer, device, ce, log_every=50)
        stats, mean_acc = evaluate(model, dl_val, device, out_dir=os.path.join(args.out_dir, "cms"))
        line = f"{epoch},{tr_loss:.4f},{mean_acc:.4f},{stats['species']['acc']:.4f},{stats['cell_org']['acc']:.4f},{stats['shape']['acc']:.4f},{stats['flagella']['acc']:.4f},{stats['chloroplast']['acc']:.4f}"
        print("  " + line)
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(line + "\n")

        # save last
        torch.save({"model": model.state_dict(), "optim": optimizer.state_dict(), "epoch": epoch},
                   os.path.join(args.out_dir, "last_teacher.pt"))
        # save best by mean_acc
        if mean_acc > best_mean_acc:
            best_mean_acc = mean_acc
            torch.save(model.state_dict(), os.path.join(args.out_dir, "best_teacher.pt"))
            print(f"  [best] mean_acc={best_mean_acc:.4f} -> saved best_teacher.pt")

        early.step(mean_acc)
        if early.should_stop:
            print(f"  Early stopping at epoch {epoch}")
            break

if __name__ == "__main__":
    main()
```

二、导出蒸馏用 Teacher logits：tools/export_teacher_logits.py
- 用 best_teacher.pt 跑 ROI 数据集，将 5 个 head 的 logits 全量导出到 .npz；
- 后续 Mask2Morph 训练可直接读取该 .npz 做 KL 蒸馏（支持温度 T）。

保存为 tools/export_teacher_logits.py

```python
# -*- coding: utf-8 -*-
import os, argparse, numpy as np, torch
from torch.utils.data import DataLoader
from ruamel.yaml import YAML
from datasets.roi_multitask_dataset import ROIMultiTaskDataset
from models.multitask_heads import MultiHeadClassifier

@torch.no_grad()
def export_logits(model, loader, device, out_npz):
    model.eval()
    paths = []
    sp_logits=[]; co_logits=[]; sh_logits=[]; fl_logits=[]; ch_logits=[]
    ids5_list=[]
    for imgs, labels, meta in loader:
        imgs = imgs.to(device)
        outs = model(imgs)
        sp_logits.append(outs['species'].detach().cpu().numpy())
        co_logits.append(outs['cell_org'].detach().cpu().numpy())
        sh_logits.append(outs['shape'].detach().cpu().numpy())
        fl_logits.append(outs['flagella'].detach().cpu().numpy())
        ch_logits.append(outs['chloroplast'].detach().cpu().numpy())
        for m in meta:
            paths.append(m['roi_path'])
            ids5_list.append(m['ids5'])
    np.savez_compressed(out_npz,
        roi_path=np.array(paths),
        species=np.concatenate(sp_logits,0),
        cell_org=np.concatenate(co_logits,0),
        shape=np.concatenate(sh_logits,0),
        flagella=np.concatenate(fl_logits,0),
        chloroplast=np.concatenate(ch_logits,0),
        ids5=np.array(ids5_list, dtype=np.int32))
    print(f"[OK] saved logits npz: {out_npz}")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--cfg", required=True)
    ap.add_argument("--roi_root", required=True)
    ap.add_argument("--weights", required=True)
    ap.add_argument("--batch_size", type=int, default=128)
    ap.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    ap.add_argument("--roi_size", type=int, default=224)
    ap.add_argument("--out_npz", type=str, default="runs/stage2_teacher/teacher_logits.npz")
    args = ap.parse_args()

    yaml = YAML()
    with open(args.cfg, 'r', encoding='utf-8') as f:
        cfg = yaml.load(f)
    mean = cfg['dataset_stats']['mean']; std = cfg['dataset_stats']['std']
    ds = ROIMultiTaskDataset(roi_root=args.roi_root, cfg_path=args.cfg, roi_size=args.roi_size, mean=mean, std=std)
    dl = DataLoader(ds, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)

    device = torch.device(args.device)
    model = MultiHeadClassifier(pretrained=False).to(device)
    model.load_state_dict(torch.load(args.weights, map_location=device))
    export_logits(model, dl, device, args.out_npz)

if __name__ == "__main__":
    main()
```

三、如何运行（建议命令）
- 训练 Teacher（用你上一步导出的 ROI 根目录）
  - python tools/train_stage2_teacher.py --cfg yolov13_transformer_unified_v2_1.yaml --roi_root tools/reports/rois_train_epoch1 --epochs 40 --batch_size 64 --weighted_sample 1 --patience 15 --device cuda
- 导出 logits
  - python tools/export_teacher_logits.py --cfg yolov13_transformer_unified_v2_1.yaml --roi_root tools/reports/rois_train_epoch1 --weights runs/stage2_teacher/best_teacher.pt --out_npz runs/stage2_teacher/teacher_logits.npz

四、验收标准
- 训练日志 log.csv 指标合理收敛；五头 val acc 基本稳定（具体数值取决于 ROI 质量和样本数，小数据阶段可先看相对改善）。
- runs/stage2_teacher/cms 里能看到五个混淆矩阵（npy + png）。
- best_teacher.pt 生成，export_teacher_logits.npz 成功导出（包含 N×C 的 logits 和对应 roi_path、ids5）。

五、下一步预告（你确认本步 OK 后）
- 切回 YOLOv13 真正的 seg 头（proto+coeff），把你当前 UnifiedSegDataset+增强接进 YOLOv13 训练器；在 RepGFPN P3/P2 注入 µScale（FiLM）并打开 P2/RepGFPN/DCNv3，小目标指标（APS）与 mAP50(M) 标准评估跑起来；
- 接一体化 Mask2Morph：在 YOLOv13 分割头后接 ROI-Align + 5 头（MLP 或 Transformer 版），训练时用 CE + KL(Teacher logits) 蒸馏；再叠加超图与软规则/几何一致性。

你先把本步 Teacher 跑一版（小数据可先 20–30 epoch），有日志/混淆矩阵/npz 后告诉我结果，我就继续给你“YOLOv13 seg 头 + 颈部 FiLM + 小目标结构”的接线与训练脚本（保证最小侵入）。