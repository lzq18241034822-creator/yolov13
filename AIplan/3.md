很棒，这一步（tools/train_unified_stage1.py + polygon→mask + 最小训练/验证可视化）已经把“数据管道、掩膜栅格化、增强闭环”打通了，作为 sanity check 是合格的。但要对齐 µSHM-YOLO 的后续路线（µScale、P2/RepGFPN/DCN、一体化五头、超图、规则/参数），当前实现还有两个关键差距需要补齐：

- 差距1：实例级训练与评测。你现在把多实例mask合并成单通道前景训练，这会弱化“实例边界/分离”监督，且不能计算实例级IoU/AP。需要升级为“实例通道训练”（N×H×W）和实例指标。
- 差距2：µScale未接入。后续的 µScale 注入、物理参数一致性、跨批次度量稳定都需要模型接收 pixel_size_um。

因此，这一步我建议做“当前脚本的增强版”，在不改你YOLOv13大框架的前提下，先完成：
- 接入 µScale（FiLM）到你当前 SimpleSegNet（快速验证 µScale 有效性）；
- 切换到实例级训练（N×H×W），使用 BCE+Dice（可选加Lovasz）；
- 增加实例级评测（mIoU/mAP 风格近似）；
- 加一个ROI导出器（从预测mask或GT mask生成 ROI crops），为后续 Stage2 蒸馏/一体化做数据准备。

等这个“小地基加强”跑稳后，我们再切换到你仓库的YOLOv13检测分割头（proto+coeff），那时FiLM会迁移到neck，P2/RepGFPN/DCN开起来，一体化与超图就会更顺利。

下面是本步的具体代码与集成说明（在你现有脚本的基础上“叠加增强”，侵入最小）。

一、添加 µScale（FiLM）到当前简易分割网络
新建 models/simple_seg_film.py（把现有 SimpleSegNet 替换为支持 µScale 的版本）
- 用 FiLM 对中间特征做通道调制（y = x*(1+γ)+β）
- 输入 pixel_scales: (B,)

```python
# models/simple_seg_film.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class PixelScaleNormalizer(nn.Module):
    def __init__(self, base_pixel_size=0.0863, method='log_ratio', eps=1e-8):
        super().__init__()
        self.base = float(base_pixel_size); self.method = method; self.eps = eps
    def forward(self, pixel_scale: torch.Tensor):
        if self.method == 'log_ratio':
            s = torch.log(pixel_scale.clamp(min=self.eps) / self.base)
        else:
            s = (pixel_scale - self.base) / (self.base + self.eps)
        return s.unsqueeze(-1)

class FiLM2d(nn.Module):
    def __init__(self, channels: int, base_pixel_size=0.0863, method='log_ratio', hidden=64):
        super().__init__()
        self.norm = PixelScaleNormalizer(base_pixel_size, method=method)
        self.mlp = nn.Sequential(nn.Linear(1, hidden), nn.SiLU(), nn.Linear(hidden, 2*channels))
        self.channels = channels
    def forward(self, x: torch.Tensor, pixel_scale: torch.Tensor):
        s = self.norm(pixel_scale)                  # (B,1)
        g = self.mlp(s)                             # (B,2C)
        gamma, beta = torch.chunk(g, 2, dim=-1)     # (B,C),(B,C)
        gamma = gamma.unsqueeze(-1).unsqueeze(-1)   # (B,C,1,1)
        beta  = beta.unsqueeze(-1).unsqueeze(-1)
        return x * (1 + gamma) + beta

class ConvBNAct(nn.Module):
    def __init__(self, c_in, c_out, k=3, s=1, p=1):
        super().__init__()
        self.cv = nn.Conv2d(c_in, c_out, k, s, p, bias=False)
        self.bn = nn.BatchNorm2d(c_out)
        self.act = nn.SiLU(True)
    def forward(self, x): return self.act(self.bn(self.cv(x)))

class SimpleSegNetFiLM(nn.Module):
    """ 轻量U-Net风格 + FiLM 注入 """
    def __init__(self, base_ch=32, out_ch=1, base_pixel_size=0.0863):
        super().__init__()
        self.film1 = FiLM2d(base_ch, base_pixel_size)
        self.film2 = FiLM2d(base_ch*2, base_pixel_size)
        self.film3 = FiLM2d(base_ch*4, base_pixel_size)

        self.enc1 = nn.Sequential(ConvBNAct(3, base_ch), ConvBNAct(base_ch, base_ch))
        self.down1= nn.MaxPool2d(2)
        self.enc2 = nn.Sequential(ConvBNAct(base_ch, base_ch*2), ConvBNAct(base_ch*2, base_ch*2))
        self.down2= nn.MaxPool2d(2)
        self.enc3 = nn.Sequential(ConvBNAct(base_ch*2, base_ch*4), ConvBNAct(base_ch*4, base_ch*4))

        self.up2  = nn.ConvTranspose2d(base_ch*4, base_ch*2, 2, 2)
        self.dec2 = nn.Sequential(ConvBNAct(base_ch*4, base_ch*2), ConvBNAct(base_ch*2, base_ch*2))
        self.up1  = nn.ConvTranspose2d(base_ch*2, base_ch, 2, 2)
        self.dec1 = nn.Sequential(ConvBNAct(base_ch*2, base_ch), ConvBNAct(base_ch, base_ch))

        self.out_head = nn.Conv2d(base_ch, out_ch, 1, 1, 0)

    def forward(self, x, pixel_scale: torch.Tensor):
        e1 = self.enc1(x); e1 = self.film1(e1, pixel_scale)
        x  = self.down1(e1)
        e2 = self.enc2(x); e2 = self.film2(e2, pixel_scale)
        x  = self.down2(e2)
        e3 = self.enc3(x); e3 = self.film3(e3, pixel_scale)

        x  = self.up2(e3)
        x  = torch.cat([x, e2], 1)
        x  = self.dec2(x)
        x  = self.up1(x)
        x  = torch.cat([x, e1], 1)
        x  = self.dec1(x)
        logits = self.out_head(x)  # (B, out_ch, H, W)
        return logits
```

二、实例级损失与指标（BCE+Dice，支持多实例通道）
新建 utils/losses_seg.py：
- 支持多实例通道的 BCEWithLogits + Dice；
- Dice 按实例通道求平均；
- 提供简单的 mIoU/mDice 计算函数用于验证。

```python
# utils/losses_seg.py
import torch
import torch.nn as nn

class BCEDiceLoss(nn.Module):
    def __init__(self, bce_weight=1.0, dice_weight=1.0, eps=1e-6):
        super().__init__()
        self.bce = nn.BCEWithLogitsLoss()
        self.bw = bce_weight; self.dw = dice_weight; self.eps = eps
    def dice_loss(self, logits, targets):
        probs = torch.sigmoid(logits)
        num = (2 * (probs * targets).sum(dim=(-1,-2)) + self.eps)
        den = (probs.pow(2).sum(dim=(-1,-2)) + targets.pow(2).sum(dim=(-1,-2)) + self.eps)
        dice = 1 - (num / den)
        # 若多通道实例：对C维求均值
        if dice.ndim == 3:  # (B,C)
            return dice.mean(dim=1).mean()
        return dice.mean()
    def forward(self, logits, targets):
        # logits: (B,C,H,W), targets: (B,C,H,W) {0,1}
        bce = self.bce(logits, targets.float())
        dice = self.dice_loss(logits, targets)
        return self.bw * bce + self.dw * dice

def batch_iou(preds_bin, targets):
    # preds_bin/targets: (B,C,H,W) 0/1
    inter = (preds_bin & targets.bool()).float().sum(dim=(-1,-2))
    union = (preds_bin | targets.bool()).float().sum(dim=(-1,-2)).clamp(min=1.0)
    iou = inter / union
    # (B,C) -> 平均
    return iou.mean()
```

三、实例掩膜栅格化 + 组装训练目标（N×H×W）
在你的 train_unified_stage1.py 里（或新建 utils/mask_ops.py），把每张图的 segments（list of K×2 归一化点）栅格化为 (C=实例数, H, W) 的 one-hot 掩膜。若你已实现，确保：
- overlap=0：返回 (C,H,W)（每实例一通道）；
- 不合并为单通道前景，以便实例级监督与指标。

示例 mask 栅格化函数（可复用你现有版）：
```python
# utils/mask_ops.py
import numpy as np
import cv2
import torch

def rasterize_segments(segs_norm, out_h, out_w):
    # segs_norm: list[np.ndarray (K,2)], 归一化到 [0,1]
    masks = []
    for s in segs_norm:
        poly = (s * np.array([out_w, out_h], dtype=np.float32)).astype(np.int32)
        m = np.zeros((out_h, out_w), dtype=np.uint8)
        cv2.fillPoly(m, [poly], 1)
        masks.append(m)
    if len(masks) == 0:
        return np.zeros((0, out_h, out_w), dtype=np.uint8)
    return np.stack(masks, axis=0)  # (C,H,W)

def pack_targets_instance(masks_np, device):
    # (C,H,W) -> torch (1,C,H,W)
    if masks_np.ndim == 2:
        masks_np = masks_np[None, ...]
    return torch.from_numpy(masks_np).unsqueeze(0).to(device)  # (1,C,H,W)
```

四、在你的 train_unified_stage1.py 中接入以上改动（核心片段）
- 用 SimpleSegNetFiLM 替代 SimpleSegNet；
- 保持 polygon 同步增强后，按 imgsz 栅格化为 instance masks；
- 用 BCEDiceLoss 训练；
- 验证时输出 mIoU/mDice；
- 把 pixel_scales 传入 model.forward。

示例关键片段（替换你现有对应位置）：
```python
# 顶部引入
from models.simple_seg_film import SimpleSegNetFiLM
from utils.losses_seg import BCEDiceLoss, batch_iou
from utils.mask_ops import rasterize_segments, pack_targets_instance

# 创建模型
model = SimpleSegNetFiLM(base_ch=32, out_ch=None, base_pixel_size=0.0863).to(device)
criterion = BCEDiceLoss(bce_weight=1.0, dice_weight=1.0)

# 训练batch内
# 经过 letterbox/affine/flip 后得到 img(segs增强后), segs(list of Kx2 norm)
# 栅格化为 (C,H,W)
masks_np = rasterize_segments(segs, imgsz, imgsz)       # (C,H,W), uint8
if masks_np.shape[0] == 0:
    # 无实例，跳过该样本；或构造空target不计loss
    continue
targets = pack_targets_instance(masks_np, device)       # (1,C,H,W)

# 前向与损失
img_tensor = torch.from_numpy(img[:, :, ::-1].copy()).permute(2,0,1).float().div_(255.0).unsqueeze(0).to(device) # BGR->RGB
logits = model(img_tensor, pixel_scale=pixel_scales[bi:bi+1].to(device))  # (1,C,H,W)，此处 out_ch 可动态=targets.size(1)，或用1x1 conv生成C通道
# 若 SimpleSegNetFiLM 输出固定1通道，你可在head前替换为 nn.Conv2d(base_ch, C, 1) 动态设置，或循环实例按单通道训练（开销大）
if logits.size(1) != targets.size(1):
    # 动态调整输出通道（简化：用1x1 conv投影）
    proj = torch.nn.Conv2d(logits.size(1), targets.size(1), 1).to(device)
    logits = proj(logits)  # 仅为跑通，正式实现请将 head 输出通道设置为C

loss = criterion(logits, targets)

# 验证指标（阈值0.5二值化）
with torch.no_grad():
    preds_bin = (torch.sigmoid(logits) > 0.5)
    miou = batch_iou(preds_bin, targets.bool())
```

注意：
- 为了不复杂化，你可以把 SimpleSegNetFiLM 的 out_head 动态生成 C 通道（根据当前样本的实例数），或者用 group-wise方式按单实例逐一训练累加loss（效率低但简单）。更优雅的方法是让 batch 内 pad 到同一最大实例数（Cmax），并用 mask 掩码忽略无效通道。
- 这是“过渡期”实例训练，后续切回 YOLOv13 seg 头会直接由 proto+coeff 对实例进行学习。

五、导出 ROI crops（供后续 Stage2/蒸馏/一体化调试）
新建 utils/roi_export.py：
- 从增强后的 segs 或预测mask中，计算每实例 bbox（归一化→像素）；按 expand_ratio 裁剪 ROI，保存到指定目录，并写出 metadata（species/cell_org 等标签用于 Teacher 训练）。

```python
# utils/roi_export.py
import os, cv2, json
import numpy as np

def bbox_from_poly_norm(poly, W, H):
    xs = (poly[:,0] * W); ys = (poly[:,1] * H)
    x0,x1 = xs.min(), xs.max(); y0,y1 = ys.min(), ys.max()
    return int(x0), int(y0), int(x1), int(y1)

def export_rois(image_bgr, segs_norm, labels_ids5, save_dir, image_stem, expand_ratio=1.2, min_size=16):
    H, W = image_bgr.shape[:2]
    os.makedirs(save_dir, exist_ok=True)
    meta = []
    for idx, poly in enumerate(segs_norm):
        x0,y0,x1,y1 = bbox_from_poly_norm(poly, W, H)
        cx = (x0+x1)/2; cy=(y0+y1)/2; w=(x1-x0); h=(y1-y0)
        w2 = int(w*expand_ratio/2); h2=int(h*expand_ratio/2)
        x0e = max(0, int(cx - w2)); y0e = max(0, int(cy - h2))
        x1e = min(W, int(cx + w2)); y1e = min(H, int(cy + h2))
        if (x1e-x0e) < min_size or (y1e-y0e) < min_size:
            continue
        crop = image_bgr[y0e:y1e, x0e:x1e]
        out_path = os.path.join(save_dir, f"{image_stem}_roi_{idx:03d}.png")
        cv2.imwrite(out_path, crop)
        ids5 = labels_ids5[idx] if idx < len(labels_ids5) else None
        meta.append({'roi_path': out_path, 'bbox_xyxy': [int(x0),int(y0),int(x1),int(y1)], 'ids5': ids5})
    meta_path = os.path.join(save_dir, f"{image_stem}_rois.json")
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    return meta_path
```

在 train/val 循环中（或单独脚本），调用 export_rois(...) 从 GT segs 或预测二值mask导出 ROI，存到 runs/roi_crops/ 下，为下一步 Teacher（两段式分类器）或一体化调试做准备。

六、这一步验收标准
- 训练日志中，loss 正常下降；验证阶段输出 mDice/mIoU；
- 可视化：叠加预测或 GT polygon 与 bbox 的图正常；
- µScale 接入有效：对比关/开 µScale 在 20~30 epoch 的 mDice/mIoU（或mask mAP近似）有正向变化（哪怕 0.5~1.0%）；
- 导出 ROI：工具能在 runs/roi_crops/ 下生成裁剪和 rois.json（含5ID标签），抽查合格。

七、为什么先做这步（而不是直接切 YOLOv13 头）
- 这是“低风险快推进”的方式：你已经有了 train_unified_stage1.py，这步只是在这个脚本上插入 µScale + 实例化训练 + ROI导出；不需要马上深入 YOLOv13 引擎内部；
- 一旦这些环节“功能正确”，接下来我们切换到 YOLOv13 原生 seg 头就只是“更换模型与损失”，数据与增强/ROI/评测都能复用，落差最小；
- 这也给你两套对照：简易FiLM + 实例损失 与 YOLO头 + proto损失。论文消融更有说服力。

下一步（你完成本步后）
- 我会提供“YOLOv13 颈部 FiLM 注入（RepGFPN P3/P2）+ 打开 P2/RepGFPN/DCNv3 开关”的源码修改点（最小侵入，含可选 runtime patch），以及“把你当前的 UnifiedSegDataset+增强 接入 YOLOv13 引擎的 SegmentTrainer”的对接脚本。随后我们就能得到真正的 mAP50(M)、APS、小目标召回，与后续一体化五头/超图紧密衔接。

如果你愿意直接跳到“YOLOv13 seg 头接入 + neck FiLM 注入”，也可以，我给你 runtime patch 方案，不用大改源码。

你先按上面的代码把当前脚本升级，跑 5~20 epoch 看看曲线和 ROI 输出是否符合预期；遇到任何报错/不匹配我来帮你对齐。