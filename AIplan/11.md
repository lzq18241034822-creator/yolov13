OK，按你要的A计划来：把 Stage1 切换到“YOLOv13 真·分割头（proto+coeff）+ 颈部FiLM + P2/RepGFPN/DCNv3”，并尽量最小侵入接到你现有仓库。下面给你可落地的代码与接线步骤，不改你已有的数据管道和Stage3。

总体思路
- 保持你当前的统一数据集管道（UnifiedSegDataset + 同步几何增强）；
- 在YOLOv13的颈部（RepGFPN）注入 µScale（FiLM）→ P3/P2输出做通道调制；
- 开启 P2 分支、确保 RepGFPN 重参数结构生效；Backbone S3/S4 打开 DCNv3（若无插件则自动退化为普通卷积）；
- 训练继续用你当前的训练脚本（或官方train.py），只是把 pixel_scale 从 DataLoader 传进 model.forward()；
- 验证通过后，Stage3 eval 和 report 直接复用。

一、新增模块：FiLM 注入
1) 新建文件 models/uscale_film.py
作用：把像素尺度 pixel_size_um 标量经 log_ratio 归一化，映射为通道级 FiLM 参数（gamma/beta），对2D特征做 y = x*(1+γ)+β。

保存为 models/uscale_film.py
```python
# -*- coding: utf-8 -*-
import torch
import torch.nn as nn

class PixelScaleNormalizer(nn.Module):
    def __init__(self, base_pixel_size=0.0863, method='log_ratio', eps=1e-8):
        super().__init__()
        self.base = float(base_pixel_size)
        self.method = method
        self.eps = eps

    def forward(self, pixel_scale: torch.Tensor):
        # pixel_scale: (B,)
        if self.method == 'log_ratio':
            s = torch.log(pixel_scale.clamp(min=self.eps) / self.base)
        else:
            s = (pixel_scale - self.base) / (self.base + self.eps)
        return s.unsqueeze(-1)  # (B,1)

class FiLM2d(nn.Module):
    def __init__(self, channels: int, base_pixel_size=0.0863, method='log_ratio', hidden=64):
        super().__init__()
        self.norm = PixelScaleNormalizer(base_pixel_size, method=method)
        self.mlp = nn.Sequential(
            nn.Linear(1, hidden),
            nn.SiLU(),
            nn.Linear(hidden, 2 * channels)
        )
        self.channels = channels

    def forward(self, x: torch.Tensor, pixel_scale: torch.Tensor):
        # x: (B, C, H, W), pixel_scale: (B,)
        s = self.norm(pixel_scale)                # (B,1)
        g = self.mlp(s)                           # (B, 2C)
        gamma, beta = torch.chunk(g, 2, dim=-1)   # (B, C), (B, C)
        gamma = gamma.unsqueeze(-1).unsqueeze(-1) # (B, C, 1, 1)
        beta  = beta.unsqueeze(-1).unsqueeze(-1)  # (B, C, 1, 1)
        return x * (1 + gamma) + beta
```

二、修改颈部：RepGFPN 注入 FiLM + P2 分支
目标：在 RepGFPN 的输出 P3/P2 上应用 FiLM；若当前实现未包含P2，则补一个轻量分支（从P3上采样或从backbone C2汇入）。

2) 修改文件 models/neck/repgfpn.py（或你仓库中RepGFPN所在文件）
- 引入 FiLM2d；
- __init__ 新增 film_p3 / film_p2；
- forward 接收 pixel_scale: torch.Tensor=None，并在 p3/p2 上调用 FiLM；
- 如果你的 RepGFPN 已有 add_p2 开关，则设默认 True；如果没有，按注释添加一个从P3上采样构建的简易 P2。

示例改动（请按你文件结构定位，命名可能不同，思路相同）
```python
# 顶部引入
from models.uscale_film import FiLM2d

class RepGFPN(nn.Module):
    def __init__(self, in_channels=(256,512,1024), out_channels=256, add_p2=True, reparam=True, **kwargs):
        super().__init__()
        self.out_channels = out_channels
        self.add_p2 = add_p2
        self.reparam = reparam

        # 你原有的RepGFPN搭建（自顶向下/自底向上/重参数化模块）...
        # 例如：lateral convs、upsample、fuse 等

        # µScale FiLM 注入器
        base_px = kwargs.get('base_pixel_size_um', 0.0863)
        method  = kwargs.get('pixel_scale_method', 'log_ratio')
        self.uscale_enabled = kwargs.get('uscale_enabled', True)
        self.film_p3 = FiLM2d(out_channels, base_pixel_size=base_px, method=method)
        if self.add_p2:
            self.film_p2 = FiLM2d(out_channels, base_pixel_size=base_px, method=method)

        # 如果你原本没有P2，这里给一条简易分支（从P3上采样）
        if self.add_p2 and not hasattr(self, 'p2_proj'):
            self.p2_proj = nn.Conv2d(out_channels, out_channels, 1, 1, 0)
            self.upsample = nn.Upsample(scale_factor=2, mode='nearest')  # P3 -> P2

    def forward(self, feats, pixel_scale: torch.Tensor=None):
        """
        feats: 通常是 backbone 输出 [C3, C4, C5] 或更多
        pixel_scale: (B,) 或 None
        """
        # 你原有的FPN逻辑，得到 p3, p4, p5（以及 p2 如果 add_p2=True）
        # 假设最终变量名为 p3, p4, p5。若没有 p2，我们从 p3 上采样得到：
        # p2 = self.p2_proj(self.upsample(p3))  # 仅在无现成P2时

        # 在输出位置注入 FiLM（仅当 pixel_scale 提供且开关开启）
        if self.uscale_enabled and (pixel_scale is not None):
            if p3 is not None:
                p3 = self.film_p3(p3, pixel_scale)
            if self.add_p2 and (p2 is not None):
                p2 = self.film_p2(p2, pixel_scale)

        if self.add_p2:
            return [p3, p4, p5, p2]
        else:
            return [p3, p4, p5]
```
注意：
- out_channels 必须和 p3/p2 的C一致（常为256），否则会报维度错；
- 如果你已有完善的 P2 from C2（更优），请用你原实现；上面仅提供无C2时的轻量备选。

三、Backbone 打开 DCNv3（可选降级）
如果你仓库已集成 DCNv3（或类似Deformable Conv），在 backbone 的 S3/S4 层加入开关；没有插件时自动退化到普通卷积（避免训练崩）。

3) 在 models/backbone/...（CSPDarknet或你的主干文件）
- 在构建 Bottleneck 或 C2f 层时，支持参数 use_dcnv3=True；
- 有 DCNv3 可用则使用 dcnv3_conv = DCNv3(...); 否则打印warning，使用 nn.Conv2d。

伪代码（仅示意）：
```python
try:
    from some_dcn_lib import DCNv3 as DCNConv
    HAS_DCNV3 = True
except Exception:
    HAS_DCNV3 = False

class Bottleneck(nn.Module):
    def __init__(self, c_in, c_out, k=3, s=1, p=1, use_dcnv3=False):
        super().__init__()
        if use_dcnv3 and HAS_DCNV3:
            self.conv = DCNConv(c_in, c_out, kernel_size=k, stride=s, padding=p)
        else:
            if use_dcnv3 and not HAS_DCNV3:
                print("[warn] DCNv3 not available; fallback to Conv2d.")
            self.conv = nn.Conv2d(c_in, c_out, k, s, p)
        self.bn = nn.BatchNorm2d(c_out)
        self.act = nn.SiLU(True)
    def forward(self, x): return self.act(self.bn(self.conv(x)))
```
- 在 Stage3/Stage4 构建时传 use_dcnv3=True 开关。

四、Model.forward 透传 pixel_scale
4) 在模型聚合类（如 models/model.py 或 yolov13/models/yolo.py 中的总 Model 类）
- 修改 forward 签名：def forward(self, x, pixel_scale=None, …)
- 调用 neck.forward(feats, pixel_scale=pixel_scale)
- head.forward(pyramids, ...) 保持原状

示例（伪）：
```python
class Model(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # self.backbone = ...
        # self.neck     = RepGFPN(...)
        # self.head     = YOLOv8SegHead(...)

    def forward(self, x, pixel_scale: torch.Tensor=None, **kwargs):
        feats = self.backbone(x)
        pyramids = self.neck(feats, pixel_scale=pixel_scale)  # 透传 µScale
        out = self.head(pyramids, **kwargs)
        return out
```

五、训练接线与配置建议
你已有 tools/train_unified_stage1.py 和 UnifiedSegDataset；这一步只需要：
- 在训练循环里把 batch 的 pixel_scales 传给 model(images, pixel_scale=pixel_scales.to(device))
- 打开 P2/RepGFPN/DCNv3 开关与超参；
- Loss 用你仓库已有的 seg loss（proto+coeff）；增强仍使用你现在 polygon 同步的 letterbox/仿射/翻转。

训练脚本修改点（你已有的 train_unified_stage1.py）
- 在拿到 batch 时：
  - outputs = model(images, pixel_scale=pixel_scales.to(device), targets=..., segments=...)
- 保持其余不动（optimizer、lr_scheduler、早停、resume 等照旧）

YAML/配置建议（与你当前v2.1一致基础上补开关）
- model.pixel_scale_conditioning.enabled = true
- model.stage1_detection.neck.add_p2 = true
- model.stage1_detection.neck.reparam = true
- model.stage1_detection.backbone.dcnv3.enabled = true
- training 超参：mosaic=0.2，mixup=0，degrees=25，轻噪声/轻模糊，loss_weights {box:7.5, dfl:1.5, mask:2.5}（与你之前一致）

六、验证与指标（切换到标准 mAP/APS）
- 训练完成后，按你已有 Stage3 推理脚本生成 predictions.json 与叠加图；
- 用 eval_stage3_report.py 评估 检测P/R/F1、cm_*.png；
- 重点关注：
  - seg mIoU（或 mAP50(M) 如你有内置评估）；
  - APS（小目标AP）：P2 开启后应显著改善；
  - 规则冲突率与 demote_rate：应渐低；
  - cell_org 混淆矩阵更对角（若同时用超图离线细化，指标会更稳）。

七、常见坑与排查
- FiLM通道不匹配：确保 FiLM2d(channels)==P3/P2的通道数（通常是256）。否则会报维度错误；
- pixel_scale 设备：pixel_scales.to(device)，与 images 同设备；
- DCNv3 无插件：按上面降级逻辑回退；部署时再考虑插件；
- P2分支：如果你已有从 C2 构建P2，请用你原实现，效果优于从P3上采样；
- 训练爆显存：P2 会增加内存与量算，batch_size 可适当降低；mask 原型数 nm 可视显存调整。

八、最小自检步骤（建议先CPU/小batch跑通）
1) 代码层面自检
- import models.uscale_film.FiLM2d 正常
- RepGFPN.forward 接受 pixel_scale 参数不报错
- Model.forward 透传 pixel_scale
- 训练脚本中 outputs = model(..., pixel_scale=pixel_scales.to(device)) 正常

2) 1~5 epoch 小跑
- loss 正常下降，无 NaN；
- 生成若干叠加图，掩膜边界与 bbox 合理；
- eval_stage3_report.py 输出 summary.csv/json 和 cm_*.png

3) 正式训练
- epochs 100~300（按你GPU）；早停；保存 best；
- 对比你之前简易分割的指标，应显著提升，尤其是 APS 与整体分割稳定性。

九、可选：保留“简易分割”为 Teacher（只在结构替换过渡期）
- 如果你担心结构替换初期不稳，可以用你简易分割 best_stage1.pt 作为 pseudo-mask 辅助（软IoU约束），但非必须。

十、完成后下一步（B计划预告）
- 将 Student 的五头挂回 YOLOv13：在分割头后接 ROI-Align → 5 头（用 best_student.pt 初始化），训练 CE + KL 蒸馏（继续用你 teacher_logits.npz）；随后把你离线的“超图细化”内化为在线图模块（或保持离线版本，二选一），并加入训练期软规则正则 & 几何一致性。

你按上面改：先把 FiLM 注入 + P2/RepGFPN/DCNv3 打开与透传跑起来。若你的 RepGFPN/CSPDarknet 文件名或类名不一样，把上述代码块对应贴进去就行；改完先跑一个 epoch 检查 shapes 和 forward 是否一致。有任何仓库路径/类名差异报错，贴具体错误栈我给你对齐补丁。