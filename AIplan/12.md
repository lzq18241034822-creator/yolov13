收到！我认真看了你现在的仓库结构和README，整体链路已经非常完整：统一标注→Stage1分割（FiLM+DCNv3，最小循环）→Stage2 Teacher/Student 蒸馏→Stage3 推理（规则+参数+可视化）→Stage3 评测与报告→超图离线细化。接下来按我们之前定的A计划，把Stage1从“简易分割网”切到“YOLOv13 真·分割头（proto+coeff）+ RepGFPN 颈部FiLM + P2/DCNv3”，拿标准 mAP/APS 指标，并和你现有的推理/评测链路无缝衔接。

这一步的任务和代码（最小侵入）

目标
- 用你现有的统一数据管道和增强，训练 YOLOv13 Seg（proto+coeff）；
- 在 RepGFPN 颈部注入 µScale（FiLM），打开 P2/DCNv3；
- 训练脚本透传 pixel_scale 到模型 forward；
- 训练结束后，直接用你已有的 Stage3 推理 + eval 脚本评测 mAP50(M)、APS、小目标指标。

1) 在 YOLOv13 颈部注入 FiLM（RepGFPN）
新增/修改文件：yolov13-main/models/neck/repgfpn.py

- 引入 FiLM2d（你已在 µSHM-YOLO/models/uscale_film.py 实现，直接 import）
- 在 __init__ 中创建 film_p3、film_p2；可从 YAML 读取 base_pixel_size 与 method；
- 在 forward 中接收 pixel_scale 并对 p3/p2 施加 FiLM；
- 若你当前 RepGFPN 尚无 P2，从 P3 上采样提供一个轻量 P2（如果你已用C2构建P2，用你原逻辑更好）。

示例（请按你仓库路径/类名对齐）：
```python
# yolov13-main/models/neck/repgfpn.py
import torch
import torch.nn as nn
# 确保能够 import 到 µSHM-YOLO 这边的 FiLM2d，必要时把路径加入 sys.path
from µSHM-YOLO.models.uscale_film import FiLM2d

class RepGFPN(nn.Module):
    def __init__(self, in_channels=(256,512,1024), out_channels=256, add_p2=True, reparam=True,
                 uscale_enabled=True, base_pixel_size_um=0.0863, pixel_scale_method='log_ratio', **kwargs):
        super().__init__()
        self.out_channels = out_channels
        self.add_p2 = add_p2
        self.reparam = reparam
        self.uscale_enabled = uscale_enabled

        # ... 你现有的 RepGFPN 构建（lateral convs, upsample, fuse, repconv) ...

        # µScale FiLM 注入器（建议注入 P3/P2）
        self.film_p3 = FiLM2d(out_channels, base_pixel_size=base_pixel_size_um, method=pixel_scale_method)
        if self.add_p2:
            self.film_p2 = FiLM2d(out_channels, base_pixel_size=base_pixel_size_um, method=pixel_scale_method)

        # 如你当前实现没有 P2，这里给一条从 P3 上采样的简易 P2（有C2更好，则删掉此备选）
        if self.add_p2 and not hasattr(self, 'p2_proj'):
            self.p2_proj = nn.Conv2d(out_channels, out_channels, 1, 1, 0)
            self.upsample = nn.Upsample(scale_factor=2, mode='nearest')

    def forward(self, feats, pixel_scale: torch.Tensor=None):
        """
        feats: backbone 输出的特征列表（通常 C3,C4,C5）
        pixel_scale: (B,) or None
        """
        # ... 你现有的FPN逻辑，最终得到 p3, p4, p5（若 add_p2 则构建 p2）
        # 示例：如果没有 P2：
        # p2 = self.p2_proj(self.upsample(p3)) if self.add_p2 else None

        # µScale FiLM 注入
        if self.uscale_enabled and (pixel_scale is not None):
            if p3 is not None:
                p3 = self.film_p3(p3, pixel_scale)
            if self.add_p2 and (p2 is not None):
                p2 = self.film_p2(p2, pixel_scale)

        return [p3, p4, p5, p2] if self.add_p2 else [p3, p4, p5]
```

2) Backbone 打开 DCNv3（库不可用则自动回退）
修改文件：yolov13-main/models/backbone/*.py（CSPDarknet 相关）
- 在 S3/S4 的瓶颈/块中支持 use_dcnv3=True；
- 若 import DCNv3 失败，则降级为 nn.Conv2d（打印warn）。

示例（伪代码）：
```python
try:
    from dcnv3 import DCNv3 as DCNConv
    HAS_DCNV3 = True
except Exception:
    HAS_DCNV3 = False
    print("[warn] DCNv3 not found, fallback to Conv2d.")

class Bottleneck(nn.Module):
    def __init__(self, c_in, c_out, kernel=3, stride=1, padding=1, use_dcnv3=False):
        super().__init__()
        if use_dcnv3 and HAS_DCNV3:
            self.conv = DCNConv(c_in, c_out, kernel_size=kernel, stride=stride, padding=padding)
        else:
            self.conv = nn.Conv2d(c_in, c_out, kernel, stride, padding, bias=False)
        self.bn = nn.BatchNorm2d(c_out)
        self.act = nn.SiLU(True)
    def forward(self, x): return self.act(self.bn(self.conv(x)))
```
在构建 S3/S4 时把 use_dcnv3=True 开关透传（按 YAML）。

3) Model.forward 透传 pixel_scale
修改总模型文件（例如 yolov13-main/models/yolo.py 或你的聚合类）：
- forward(self, x, pixel_scale=None, …)
- backbone→neck(feats, pixel_scale=pixel_scale)→head

```python
class Model(nn.Module):
    def __init__(self, ...):
        super().__init__()
        # self.backbone = ...
        # self.neck = RepGFPN(..., uscale_enabled=cfg['model']['pixel_scale_conditioning']['enabled'], ...)
        # self.head = YOLOv8SegHead(...)

    def forward(self, x, pixel_scale: torch.Tensor=None, **kwargs):
        feats = self.backbone(x)
        pyramids = self.neck(feats, pixel_scale=pixel_scale)
        out = self.head(pyramids, **kwargs)
        return out
```

4) 训练脚本：tools/train_yolov13_stage1.py（用你的数据集/增强）
- 使用 µSHM-YOLO/datasets/unified_seg_dataset.UnifiedSegDataset（你现成的）；
- 调用 YOLOv13 模型，传 pixel_scales（来自TXT头部）；
- Loss 使用你仓库的 SegLoss（若已有），否则暂保留现有损失。你已有 Stage3 评测，不强依赖 Ultralytics 的内置评估。

保存为 µSHM-YOLO/tools/train_yolov13_stage1.py（按你现有训练模板改最少）：
```python
# -*- coding: utf-8 -*-
import os, argparse, numpy as np, torch, torch.nn as nn
from torch.utils.data import DataLoader
from ruamel.yaml import YAML

# 你的数据集/增强
from datasets.unified_seg_dataset import UnifiedSegDataset, collate_fn_unified
from utils.aug_poly import letterbox_with_segments, random_affine_with_segments, hflip_with_segments
from utils.mask_ops import rasterize_segments, pack_targets_instance
from utils.losses_seg import BCEDiceLoss  # 若你有YOLO seg 专用loss则替换

# 引入你YOLOv13模型的构建（按你repo实际路径）
from yolov13_main.models.yolo import Model as YOLOSegModel

def set_seed(seed=42):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--cfg", required=True)
    ap.add_argument("--data_root", required=True)
    ap.add_argument("--epochs", type=int, default=100)
    ap.add_argument("--batch_size", type=int, default=8)
    ap.add_argument("--imgsz", type=int, default=640)
    ap.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    ap.add_argument("--workers", type=int, default=2)
    ap.add_argument("--split_dir", type=str, default=None)
    args = ap.parse_args()

    set_seed(42)
    device = torch.device(args.device)

    yaml = YAML()
    with open(args.cfg, 'r', encoding='utf-8') as f:
        cfg = yaml.load(f)
    data_root = args.data_root
    imgsz = args.imgsz
    skip_ids = cfg['training']['stage1_training']['dataloader']['stage1_class_filter']['skip_species_ids']

    # 构建数据集（固定划分）
    train_list = os.path.join(args.split_dir, "train.txt") if args.split_dir else None
    val_list   = os.path.join(args.split_dir, "val.txt")   if args.split_dir else None
    train_set = UnifiedSegDataset(data_root, 'train', args.cfg, imgsz, skip_ids, id_list_file=train_list)
    val_set   = UnifiedSegDataset(data_root, 'train', args.cfg, imgsz, skip_ids, id_list_file=val_list)

    dl_train = DataLoader(train_set, batch_size=args.batch_size, shuffle=True,  num_workers=args.workers, pin_memory=True, collate_fn=collate_fn_unified)
    dl_val   = DataLoader(val_set,   batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, collate_fn=collate_fn_unified)

    # 构建YOLOv13分割模型（以YAML开关控制 P2/RepGFPN/DCNv3/FiLM）
    uscale_cfg = cfg['model'].get('pixel_scale_conditioning', {})
    neck_cfg   = cfg['model']['stage1_detection'].get('neck', {})
    dcn_cfg    = cfg['model']['stage1_detection']['backbone']['dcnv3']

    model = YOLOSegModel(
        # 你仓库里的构造参数：backbone/neck/head 读取YAML开关
        # 示例：backbone={'use_dcnv3': dcn_cfg['enabled'], ...},
        #       neck={'type':'RepGFPN','add_p2': neck_cfg.get('add_p2', True), 'uscale_enabled': uscale_cfg.get('enabled', True),
        #             'base_pixel_size_um': cfg['microscope']['pixel_size_um'], 'pixel_scale_method': uscale_cfg.get('norm','log_ratio')}
    ).to(device)

    # 损失（若你repo有YOLO的seg loss，优先用；否则暂用 BCE+Dice）
    criterion = BCEDiceLoss(bce_weight=0.5, dice_weight=0.5).to(device)
    optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=5e-4)

    best = -1
    for epoch in range(1, args.epochs+1):
        model.train(); losses=[]
        for imgs_u8, labels, segments, paths, pixel_scales, shapes in dl_train:
            batch_imgs=[]; batch_targets=[]
            for bi in range(imgs_u8.shape[0]):
                img = imgs_u8[bi].numpy(); segs = segments[bi]
                img, segs, _, _ = letterbox_with_segments(img, segs, new_shape=(imgsz, imgsz))
                img, segs = random_affine_with_segments(img, segs, degrees=10, translate=0.1, scale=0.3, shear=0)
                if np.random.rand() < 0.5:
                    img, segs = hflip_with_segments(img, segs)
                # 栅格化为 (C,H,W)
                masks_np = rasterize_segments(segs, imgsz, imgsz)  # (C,H,W)
                if masks_np.shape[0] == 0:
                    continue
                # 组装
                x = torch.from_numpy(img[:,:,::-1].copy()).permute(2,0,1).float().div(255.0).unsqueeze(0)  # BGR->RGB
                y = pack_targets_instance(masks_np, device="cpu")  # (1,C,H,W) 暂CPU合并后再to(device)
                batch_imgs.append(x); batch_targets.append(y)
            if not batch_imgs:
                continue
            images = torch.cat(batch_imgs, dim=0).to(device)                       # (B,3,H,W)
            targets = torch.cat(batch_targets, dim=0).to(device)                   # (B,C,H,W)
            px = pixel_scales[:images.size(0)].to(device)

            # 前向（注意：YOLOv13分割头通常输出 proto/coeff，我们此处先用占位 logits，若你repo已有完整seg loss请替换）
            logits = model(images, pixel_scale=px)  # 期望输出 (B,C,H,W) 或通过你的head解码为实例掩膜
            # 如果你的head输出proto/coeff，请改为你repo的 seg loss 组合；以下仅为占位
            if logits.size() != targets.size():
                # 需要你repo的真实head输出与训练损失；这里只是简化占位
                logits = nn.functional.interpolate(logits, size=targets.shape[-2:], mode='bilinear', align_corners=False)
            loss = criterion(logits, targets)
            optim.zero_grad(set_to_none=True); loss.backward(); optim.step()
            losses.append(loss.item())

        # 简单 val（可用你 Stage3 eval 做离线评估）
        val_loss = float(np.mean(losses)) if losses else 0.0
        print(f"Epoch {epoch}/{args.epochs} train_loss={val_loss:.4f}")
        # 保存best（用最低loss占位，实际建议接入你的mAP计算）
        score = -val_loss
        if score > best:
            best = score
            os.makedirs("runs/stage1_yolov13", exist_ok=True)
            torch.save({"model": model.state_dict(), "epoch": epoch},
                       "runs/stage1_yolov13/best.pt")
            print("  [best] saved runs/stage1_yolov13/best.pt")

if __name__ == "__main__":
    main()
```
说明：
- 上面训练脚本中的 “logits/criterion” 是占位：如果你仓库已经有 YOLOv13 分割头（proto+coeff）与对应的 loss（Mask BCE + coeff/L1/IOU 等），请直接替换 criterion / logits 部分为你的真实实现（调用 head 的训练接口）。
- 如果你倾向直接调用Ultralytics的 SegmentTrainer，我也能提供一个 train_ultra_stage1.py，把你 UnifiedSegDataset 接成 ultralytics 的 batch schema（masks/cls/segments），用官方 loss 和 metric，最快拿到 mAP50(M)/APS。

5) 运行与验收
- 训练：
  - python µSHM-YOLO\tools\train_yolov13_stage1.py --cfg µSHM-YOLO\yolov13_transformer_unified_v2_1.yaml --data_root µSHM-YOLO\samples --epochs 150 --batch_size 8 --imgsz 640 --device cuda --split_dir µSHM-YOLO\splits
- 推理与评测（复用你已有脚本）：
  - infer：python µSHM-YOLO\tools\infer_stage3_pipeline.py --cfg ... --stage1_weights runs\stage1_yolov13\best.pt --student_weights runs\stage2_student\best_student.pt --out_dir runs\infer_stage3_y13
  - eval：python µSHM-YOLO\tools\eval_stage3_report.py --data_root ... --pred_json runs\infer_stage3_y13\predictions.json --imgsz 640 --iou_thr 0.5 --save_dir runs\eval_stage3_y13 --plot 1
- 验收标准：
  - 训练能稳定收敛（loss下降），不会报形状错误；
  - eval 的 seg 指标与你简易分割相比显著提升，APS 改善明显；
  - 混淆矩阵与冲突率合理下降。

小结与下一步
- 完成这步后，你就把Stage1切到了“标准mAP”的通道，后续论文指标更规范；
- 接下来 B计划：把 Student 五头挂回 YOLOv13 分割头后（ROI-Align → 5头），训练 CE + KL（Teacher/Student），再叠加超图在线版与软规则/几何一致性。

如果你更希望直接走 Ultralytics 的 SegmentTrainer（更快拿 mAP 和 APS），我可以把你 UnifiedSegDataset 对接成 ultralytics 的 batch schema，基本不改你的仓库，给你 train_ultra_stage1.py 一键脚本。你确定路线我就按你选的给对应补丁。