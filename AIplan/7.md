太棒了！你已经把 Stage1 地基打实 + Stage2 Teacher 最小闭环也跑通了。按我们的路线，下一步进入“蒸馏学生（Mask2Morph-lite）”，先在 ROI 层面用 Teacher 的 logits 做蒸馏，把五头分类（species/cell_org/shape/flagella/chloroplast）稳定起来。这样后面把学生头挂回 YOLOv13（ROI-Align → 5头）就很顺了。

本步目标
- 用你导出的 ROI 与 teacher_logits.npz，训练一个 Student 多头分类器（轻量或与Teacher同结构），损失=CE + λkd·KL(Teacher, Student)（温度T）。
- 完整支持：resume/早停、加权采样（按species）、固定划分（可沿用当前ROI目录或随机8:2）。
- 导出 best_student.pt，并可导出学生 logits（后续挂接到一体化头做warm start）。

交付内容
- 新增数据集：datasets/roi_with_teacher_logits.py（ROI + Teacher logits）
- 蒸馏工具：utils/distill.py（带温度的 KL）
- 训练脚本：tools/train_mask2morph_distill.py（Student 训练 + 蒸馏）
- 可选：导出 Student logits：tools/export_student_logits.py

一、数据集：datasets/roi_with_teacher_logits.py
把 rois.json 与 teacher_logits.npz 对齐（按 roi_path 映射），返回图像Tensor、五头标签、五头Teacher logits。

保存为 datasets/roi_with_teacher_logits.py
```python
# -*- coding: utf-8 -*-
import os, json, glob
from typing import List, Dict, Any
import numpy as np
import torch
from torch.utils.data import Dataset
import cv2
from torchvision import transforms
from ruamel.yaml import YAML

# 复用这些映射逻辑，确保与 YAML 一致
def map_cell_org_global_to_local(g: int) -> int:
    if g in (10,11):
        g = 44
    return {7:0, 8:1, 9:2, 44:3}[g]
def map_shape_global_to_local(g: int) -> int:
    return {12:0,13:1,14:2,15:3,16:4,17:5,18:6,19:7,20:8,21:9}[g]
def map_flagella_global_to_local(g: int) -> int:
    return {22:0,23:1,24:2,25:3,26:4}[g]
def map_chloro_global_to_local(g: int) -> int:
    return {27:0,28:1,29:2,30:3}[g]

def imread_unicode(path: str):
    data = np.fromfile(path, dtype=np.uint8)
    img = cv2.imdecode(data, cv2.IMREAD_COLOR)
    return img

def load_yaml(path: str) -> dict:
    y = YAML()
    with open(path, 'r', encoding='utf-8') as f:
        return y.load(f)

class ROIWithTeacherDataset(Dataset):
    def __init__(self,
                 roi_root: str,
                 cfg_path: str,
                 teacher_npz: str,
                 roi_size: int = 224):
        super().__init__()
        cfg = load_yaml(cfg_path)
        mean = cfg['dataset_stats']['mean']
        std  = cfg['dataset_stats']['std']
        self.tfm = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((roi_size, roi_size)),
            transforms.Normalize(mean=mean, std=std),
        ])
        # 收集 rois.json
        cand = glob.glob(os.path.join(roi_root, "*.json"))
        if not cand:
            cand = glob.glob(os.path.join(roi_root, "**", "*.json"), recursive=True)
        items = []
        for jp in cand:
            arr = json.load(open(jp, 'r', encoding='utf-8'))
            for rec in arr:
                if ('roi_path' in rec) and ('ids5' in rec) and os.path.exists(rec['roi_path']):
                    items.append(rec)
        if not items:
            raise RuntimeError(f"No ROI entries found under {roi_root}")

        # 加载 teacher logits_npz 并建立 path→index 映射
        npz = np.load(teacher_npz, allow_pickle=True)
        paths = npz['roi_path']
        self.map_idx = {p: i for i, p in enumerate(paths)}
        self.teacher = {
            'species': npz['species'],        # (N,45)
            'cell_org': npz['cell_org'],      # (N,4)
            'shape': npz['shape'],            # (N,10)
            'flagella': npz['flagella'],      # (N,5)
            'chloroplast': npz['chloroplast'] # (N,4)
        }
        # 过滤没有teacher logits 的样本
        self.items: List[Dict[str,Any]] = [rec for rec in items if rec['roi_path'] in self.map_idx]
        if not self.items:
            raise RuntimeError("ROI items exist but none aligned with teacher logits; check paths in npz vs rois.json")

    def __len__(self): return len(self.items)

    def __getitem__(self, i: int):
        rec = self.items[i]
        p = rec['roi_path']
        img = imread_unicode(p)
        if img is None:
            raise FileNotFoundError(f"Fail to read {p}")
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        x = self.tfm(img)  # (3, H, W)

        sp, co, sh, fl, ch = rec['ids5']
        labels = {
            'species': torch.tensor(int(sp), dtype=torch.long),
            'cell_org': torch.tensor(map_cell_org_global_to_local(int(co)), dtype=torch.long),
            'shape': torch.tensor(map_shape_global_to_local(int(sh)), dtype=torch.long),
            'flagella': torch.tensor(map_flagella_global_to_local(int(fl)), dtype=torch.long),
            'chloroplast': torch.tensor(map_chloro_global_to_local(int(ch)), dtype=torch.long),
        }
        j = self.map_idx[p]
        t_logits = {
            'species': torch.from_numpy(self.teacher['species'][j]).float(),
            'cell_org': torch.from_numpy(self.teacher['cell_org'][j]).float(),
            'shape': torch.from_numpy(self.teacher['shape'][j]).float(),
            'flagella': torch.from_numpy(self.teacher['flagella'][j]).float(),
            'chloroplast': torch.from_numpy(self.teacher['chloroplast'][j]).float(),
        }
        meta = {'roi_path': p, 'ids5': rec['ids5']}
        return x, labels, t_logits, meta
```

二、蒸馏工具：utils/distill.py
温度T 的 KL 蒸馏（多头通用）。

保存为 utils/distill.py
```python
# -*- coding: utf-8 -*-
import torch
import torch.nn.functional as F

def kl_div_with_temperature(student_logits: torch.Tensor,
                            teacher_logits: torch.Tensor,
                            T: float = 2.0,
                            reduction: str = 'batchmean'):
    # student_logits/teacher_logits: (B,C)
    log_p_s = F.log_softmax(student_logits / T, dim=1)
    p_t     = F.softmax(teacher_logits / T, dim=1)
    return F.kl_div(log_p_s, p_t, reduction=reduction) * (T * T)
```

三、训练脚本（蒸馏）：tools/train_mask2morph_distill.py
- Student 结构：直接复用 MultiHeadClassifier（也可选 ResNet18 以更轻）。
- Loss = Σ CE + λkd·Σ KL_T
- 支持 weighted_sample / resume / early stop / safe_loop（避免 DataLoader 问题时可退回单线程取样）
- 保存 best_student.pt 与 log.csv；可选导出 Student 混淆矩阵。

保存为 tools/train_mask2morph_distill.py
```python
# -*- coding: utf-8 -*-
import os, argparse, numpy as np, torch, torch.nn as nn
from torch.utils.data import DataLoader, random_split, WeightedRandomSampler
from ruamel.yaml import YAML
from datasets.roi_with_teacher_logits import ROIWithTeacherDataset
from models.multitask_heads import MultiHeadClassifier
from utils.metrics_cls import accuracy, macro_f1, confusion_matrix
from utils.distill import kl_div_with_temperature

def set_seed(seed=42):
    import random
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

class EarlyStopping:
    def __init__(self, patience=15, min_delta=1e-4):
        self.patience=patience; self.min_delta=min_delta
        self.best=None; self.count=0; self.should_stop=False
    def step(self, metric):
        if self.best is None or metric > self.best + self.min_delta:
            self.best = metric; self.count=0
        else:
            self.count += 1
            if self.count >= self.patience:
                self.should_stop=True

class LabelSmoothingCE(nn.Module):
    def __init__(self, eps=0.1):
        super().__init__(); self.eps=eps; self.lsm=nn.LogSoftmax(dim=1)
    def forward(self, logits, target):
        C = logits.size(1)
        logp = self.lsm(logits)
        with torch.no_grad():
            true = torch.zeros_like(logp)
            true.fill_(self.eps/(C-1))
            true.scatter_(1, target.unsqueeze(1), 1-self.eps)
        return torch.mean(torch.sum(-true * logp, dim=1))

def compute_species_weights(ds):
    from collections import Counter
    cnt = Counter()
    for i in range(len(ds)):
        _, labels, _, _ = ds[i]
        cnt[int(labels['species'])] += 1
    total = sum(cnt.values())
    cw = {k: total / (v + 1e-6) for k, v in cnt.items()}
    weights = []
    for i in range(len(ds)):
        _, labels, _, _ = ds[i]
        weights.append(cw[int(labels['species'])])
    return torch.DoubleTensor(weights)

@torch.no_grad()
def evaluate(model, loader, device, out_dir=None):
    model.eval()
    heads = {'species':45, 'cell_org':4, 'shape':10, 'flagella':5, 'chloroplast':4}
    stats = {k: {'acc':0,'f1':0,'cm':np.zeros((C,C),dtype=np.int64)} for k,C in heads.items()}
    total=0
    for imgs, labels, _, meta in loader:
        imgs = imgs.to(device)
        sp = labels['species'].to(device)
        co = labels['cell_org'].to(device)
        sh = labels['shape'].to(device)
        fl = labels['flagella'].to(device)
        ch = labels['chloroplast'].to(device)
        outs = model(imgs)
        for k, targ, C in [('species',sp,45),('cell_org',co,4),('shape',sh,10),('flagella',fl,5),('chloroplast',ch,4)]:
            acc = accuracy(outs[k], targ); f1 = macro_f1(outs[k], targ, C); cm = confusion_matrix(outs[k], targ, C)
            stats[k]['acc'] += acc * imgs.size(0); stats[k]['f1'] += f1 * imgs.size(0); stats[k]['cm'] += cm
        total += imgs.size(0)
    for k in stats.keys():
        stats[k]['acc'] /= total; stats[k]['f1'] /= total
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)
        for k in stats.keys():
            np.save(os.path.join(out_dir, f"cm_{k}.npy"), stats[k]['cm'])
    mean_acc = np.mean([stats[k]['acc'] for k in stats.keys()])
    return stats, float(mean_acc)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--cfg", required=True)
    ap.add_argument("--roi_root", required=True)
    ap.add_argument("--teacher_npz", required=True)
    ap.add_argument("--epochs", type=int, default=40)
    ap.add_argument("--batch_size", type=int, default=64)
    ap.add_argument("--lr", type=float, default=5e-4)
    ap.add_argument("--weight_decay", type=float, default=1e-4)
    ap.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    ap.add_argument("--roi_size", type=int, default=224)
    ap.add_argument("--out_dir", type=str, default="runs/stage2_student")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--weighted_sample", type=int, default=1)
    ap.add_argument("--resume", type=str, default=None)
    ap.add_argument("--patience", type=int, default=15)
    ap.add_argument("--label_smooth", type=float, default=0.1)
    ap.add_argument("--kd_weight", type=float, default=0.5)
    ap.add_argument("--kd_T", type=float, default=2.0)
    ap.add_argument("--safe_loop", type=int, default=0)
    args = ap.parse_args()

    set_seed(args.seed)
    device = torch.device(args.device)

    ds = ROIWithTeacherDataset(args.roi_root, args.cfg, args.teacher_npz, roi_size=args.roi_size)
    n_total = len(ds); n_val = max(1, int(0.2*n_total)); n_train = n_total - n_val
    if args.safe_loop:
        # 不用 DataLoader，单线程 loop
        print("[safe_loop] enabled; no DataLoader workers.")
    else:
        from torch.utils.data import random_split
        ds_train, ds_val = random_split(ds, [n_train, n_val], generator=torch.Generator().manual_seed(args.seed))
        if args.weighted_sample:
            wts = compute_species_weights(ds_train.dataset if hasattr(ds_train,'dataset') else ds_train)
            sampler = WeightedRandomSampler(weights=wts, num_samples=len(wts), replacement=True)
            dl_train = DataLoader(ds_train, batch_size=args.batch_size, sampler=sampler, num_workers=0, pin_memory=False)
        else:
            dl_train = DataLoader(ds_train, batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=False)
        dl_val = DataLoader(ds_val, batch_size=args.batch_size, shuffle=False, num_workers=0, pin_memory=False)

    model = MultiHeadClassifier(pretrained=True).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    ce = LabelSmoothingCE(eps=args.label_smooth).to(device)

    os.makedirs(args.out_dir, exist_ok=True)
    log_path = os.path.join(args.out_dir, "log.csv")
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write("epoch,train_loss,val_mean_acc,val_species_acc,val_cell_org_acc,val_shape_acc,val_flagella_acc,val_chloro_acc\n")

    start_epoch = 1; best_mean_acc = -1
    if args.resume and os.path.exists(args.resume):
        ckpt = torch.load(args.resume, map_location=device)
        model.load_state_dict(ckpt['model']); optimizer.load_state_dict(ckpt['optim'])
        start_epoch = ckpt.get('epoch', 0) + 1
        print(f"[resume] -> epoch {start_epoch}")
    early = EarlyStopping(patience=args.patience, min_delta=1e-4)

    # safe loop 简化实现
    if args.safe_loop:
        # 手动split
        idxs = np.arange(len(ds)); np.random.seed(args.seed); np.random.shuffle(idxs)
        val_idx = idxs[:n_val]; tr_idx = idxs[n_val:]
        def batch_iter(idxs, bs):
            for i in range(0, len(idxs), bs):
                yield [ds[j] for j in idxs[i:i+bs]]

    for epoch in range(start_epoch, args.epochs+1):
        print(f"\nEpoch {epoch}/{args.epochs}")
        model.train(); losses=[]
        if args.safe_loop:
            for batch in batch_iter(tr_idx, args.batch_size):
                imgs = torch.stack([b[0] for b in batch]).to(device)
                labels = {k: torch.stack([b[1][k] for b in batch]).to(device) for k in ['species','cell_org','shape','flagella','chloroplast']}
                tlogits = {k: torch.stack([b[2][k] for b in batch]).to(device) for k in ['species','cell_org','shape','flagella','chloroplast']}
                outs = model(imgs)
                loss_ce = (ce(outs['species'], labels['species']) +
                           ce(outs['cell_org'], labels['cell_org']) +
                           ce(outs['shape'], labels['shape']) +
                           ce(outs['flagella'], labels['flagella']) +
                           ce(outs['chloroplast'], labels['chloroplast']))
                loss_kd = (kl_div_with_temperature(outs['species'], tlogits['species'], args.kd_T) +
                           kl_div_with_temperature(outs['cell_org'], tlogits['cell_org'], args.kd_T) +
                           kl_div_with_temperature(outs['shape'], tlogits['shape'], args.kd_T) +
                           kl_div_with_temperature(outs['flagella'], tlogits['flagella'], args.kd_T) +
                           kl_div_with_temperature(outs['chloroplast'], tlogits['chloroplast'], args.kd_T))
                loss = loss_ce + args.kd_weight * loss_kd
                optimizer.zero_grad(set_to_none=True); loss.backward(); optimizer.step()
                losses.append(loss.item())
        else:
            for imgs, labels, tlogits, meta in dl_train:
                imgs = imgs.to(device)
                for k in labels: labels[k]=labels[k].to(device)
                for k in tlogits: tlogits[k]=tlogits[k].to(device)
                outs = model(imgs)
                loss_ce = (ce(outs['species'], labels['species']) +
                           ce(outs['cell_org'], labels['cell_org']) +
                           ce(outs['shape'], labels['shape']) +
                           ce(outs['flagella'], labels['flagella']) +
                           ce(outs['chloroplast'], labels['chloroplast']))
                loss_kd = (kl_div_with_temperature(outs['species'], tlogits['species'], args.kd_T) +
                           kl_div_with_temperature(outs['cell_org'], tlogits['cell_org'], args.kd_T) +
                           kl_div_with_temperature(outs['shape'], tlogits['shape'], args.kd_T) +
                           kl_div_with_temperature(outs['flagella'], tlogits['flagella'], args.kd_T) +
                           kl_div_with_temperature(outs['chloroplast'], tlogits['chloroplast'], args.kd_T))
                loss = loss_ce + args.kd_weight * loss_kd
                optimizer.zero_grad(set_to_none=True); loss.backward(); optimizer.step()
                losses.append(loss.item())

        tr_loss = float(np.mean(losses)) if losses else 0.0

        # 验证
        if args.safe_loop:
            # 简易eval
            model.eval()
            accs = {'species':0,'cell_org':0,'shape':0,'flagella':0,'chloroplast':0}
            total=0
            with torch.no_grad():
                for batch in batch_iter(val_idx, args.batch_size):
                    imgs = torch.stack([b[0] for b in batch]).to(device)
                    labels = {k: torch.stack([b[1][k] for b in batch]).to(device) for k in accs.keys()}
                    outs = model(imgs)
                    accs['species'] += accuracy(outs['species'], labels['species'])*imgs.size(0)
                    accs['cell_org'] += accuracy(outs['cell_org'], labels['cell_org'])*imgs.size(0)
                    accs['shape'] += accuracy(outs['shape'], labels['shape'])*imgs.size(0)
                    accs['flagella'] += accuracy(outs['flagella'], labels['flagella'])*imgs.size(0)
                    accs['chloroplast'] += accuracy(outs['chloroplast'], labels['chloroplast'])*imgs.size(0)
                    total += imgs.size(0)
            for k in accs: accs[k] /= total
            mean_acc = float(np.mean(list(accs.values())))
            stats = {k:{'acc':accs[k]} for k in accs}
        else:
            stats, mean_acc = evaluate(model, dl_val, device, out_dir=os.path.join(args.out_dir, "cms"))

        # 记录与保存
        print(f"  loss={tr_loss:.4f} mean_acc={mean_acc:.4f} sp={stats['species']['acc']:.4f} co={stats['cell_org']['acc']:.4f}")
        with open(os.path.join(args.out_dir, "log.csv"), 'a', encoding='utf-8') as f:
            f.write(f"{epoch},{tr_loss:.4f},{mean_acc:.4f},{stats['species']['acc']:.4f},{stats['cell_org']['acc']:.4f},"
                    f"{stats.get('shape',{}).get('acc',0):.4f},{stats.get('flagella',{}).get('acc',0):.4f},{stats.get('chloroplast',{}).get('acc',0):.4f}\n")

        torch.save({"model": model.state_dict(), "optim": optimizer.state_dict(), "epoch": epoch},
                   os.path.join(args.out_dir, "last_student.pt"))
        if mean_acc > best_mean_acc:
            best_mean_acc = mean_acc
            torch.save(model.state_dict(), os.path.join(args.out_dir, "best_student.pt"))
            print(f"  [best] mean_acc={best_mean_acc:.4f} -> saved best_student.pt")

        early.step(mean_acc)
        if early.should_stop:
            print(f"  Early stopping at epoch {epoch}")
            break

if __name__ == "__main__":
    main()
```

四、可选：导出学生 logits（供后续一体化 warm start）
保存为 tools/export_student_logits.py（与 teacher 的导出几乎一致，载入 best_student.pt 即可）。你可以直接复制 export_teacher_logits.py 改名，加载 best_student.pt。

五、如何运行
- 蒸馏学生（安全模式，避免Dataloader问题）：
  - python tools/train_mask2morph_distill.py --cfg yolov13_transformer_unified_v2_1.yaml --roi_root tools/reports/rois_train_epoch1 --teacher_npz runs/stage2_teacher/teacher_logits.npz --epochs 30 --batch_size 64 --kd_weight 0.5 --kd_T 2.0 --label_smooth 0.1 --safe_loop 1 --device cpu
- 正常模式（性能更好）：
  - python tools/train_mask2morph_distill.py --cfg yolov13_transformer_unified_v2_1.yaml --roi_root tools/reports/rois_train_epoch1 --teacher_npz runs/stage2_teacher/teacher_logits.npz --epochs 40 --batch_size 64 --kd_weight 0.5 --kd_T 2.0 --label_smooth 0.1 --weighted_sample 1 --device cuda

六、验收标准
- runs/stage2_student/log.csv 指标持续改善，val 平均准确率有提升（与 Teacher 相近或略低是合理，蒸馏可逼近）；
- best_student.pt 保存；cms/ 下有混淆矩阵（开启非安全模式时）。

七、下一步预告
- 将学生头挂回 YOLOv13：在分割头后接 ROI-Align（从 P2/P3/P4 按目标尺寸选择），把训练好的 5 头初始化（warm start），训练 CE + KL（继续蒸馏），先用 GT ROI，逐步混入预测 ROI；
- 同时切换 Stage1 到 YOLOv13 真正 seg 头（proto+coeff），在 RepGFPN P3/P2 注入 FiLM 并打开 P2/RepGFPN/DCNv3，获得标准 mAP50(M)/APS；
- 然后加超图与软规则/几何一致性，形成完整 µSHM-YOLO。

你先把本步蒸馏学生跑一版（安全模式 epochs=5~10 看曲线，OK 后切换正常模式 epochs=30~40）。如果一切顺利，我就给你“YOLOv13 seg 头 + neck-FiLM + P2/RepGFPN/DCNv3”的接线改动（最小侵入补丁），把 Stage1 指标切到标准 mAP50(M)/APS。